{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 64 # 64x64 images \n",
        "EPOCHS = 50 # number of epochs \n",
        "DATASET_DIR = \"path/to/dataset\" # Directory of the images [[ FOLDER FULL OF IMAGES ]]"
      ],
      "metadata": {
        "id": "_XS4WAPBC2xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZGEQO4YCVCR"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/diffusers.git#egg=diffusers[training]\n",
        "!pip install \"ipywidgets>=7,<8\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "RVcAnrp7Cp0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo apt -qq install git-lfs\n",
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "YiR5MFaECrIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    image_size = IMAGE_SIZE  # the generated image resolution\n",
        "    train_batch_size = 16\n",
        "    eval_batch_size = 16  # how many images to sample during evaluation\n",
        "    num_epochs = EPOCHS\n",
        "    gradient_accumulation_steps = 1\n",
        "    learning_rate = 1e-4\n",
        "    lr_warmup_steps = 500\n",
        "    save_image_epochs = 5\n",
        "    save_model_epochs = 10\n",
        "    mixed_precision = 'fp16'  # `no` for float32, `fp16` for automatic mixed precision\n",
        "    output_dir = 'ddpm-local-pixelart'  # the model namy locally and on the HF Hub\n",
        "\n",
        "    push_to_hub = True  # whether to upload the saved model to the HF Hub\n",
        "    hub_private_repo = True  \n",
        "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
        "    seed = 0\n",
        "    dataset_name = \"imagefolder\"\n",
        "\n",
        "config = TrainingConfig()"
      ],
      "metadata": {
        "id": "UZuuXjCCC1W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "config.dataset_name = \"imagefolder\"\n",
        "dataset = load_dataset(config.dataset_name, data_dir=DATASET_DIR)"
      ],
      "metadata": {
        "id": "big0uPlvDNST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "9_iFWOO6EEpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "for i, image in enumerate(dataset['train'][:4][\"image\"]):\n",
        "    axs[i].imshow(image)\n",
        "    axs[i].set_axis_off()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "G2AbEJ1sEGqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(examples):\n",
        "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
        "    return {\"images\": images}\n",
        "\n",
        "dataset.set_transform(transform)"
      ],
      "metadata": {
        "id": "x8NJRJ0PEkuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "for i, image in enumerate(dataset['train'][:4][\"images\"]):\n",
        "    axs[i].imshow(image.permute(1, 2, 0).numpy() / 2 + 0.5)\n",
        "    axs[i].set_axis_off()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "XB6DqAgVEodQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset['train'], batch_size=config.train_batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "9NZ7ZNHVEqn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DModel\n",
        "\n",
        "\n",
        "model = UNet2DModel(\n",
        "    sample_size=config.image_size,  # the target image resolution\n",
        "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
        "    out_channels=3,  # the number of output channels\n",
        "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
        "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channes for each UNet block\n",
        "    down_block_types=( \n",
        "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
        "        \"DownBlock2D\", \n",
        "        \"DownBlock2D\", \n",
        "        \"DownBlock2D\", \n",
        "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
        "        \"DownBlock2D\",\n",
        "    ), \n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
        "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
        "        \"UpBlock2D\", \n",
        "        \"UpBlock2D\", \n",
        "        \"UpBlock2D\", \n",
        "        \"UpBlock2D\"  \n",
        "      ),\n",
        ")"
      ],
      "metadata": {
        "id": "9jSM4ZOREso9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image = dataset['train'][0]['images'].unsqueeze(0)\n",
        "print('Input shape:', sample_image.shape)"
      ],
      "metadata": {
        "id": "q1u5v2HsEuOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Output shape:', model(sample_image, timestep=0).sample.shape)"
      ],
      "metadata": {
        "id": "4ooPMMctEweU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DDPMScheduler\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, tensor_format=\"pt\")"
      ],
      "metadata": {
        "id": "cIxNUlH_EynF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "noise = torch.randn(sample_image.shape)\n",
        "timesteps = torch.LongTensor([50])\n",
        "noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
        "\n",
        "Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])"
      ],
      "metadata": {
        "id": "DGtj6ObQE0It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "noise_pred = model(noisy_image, timesteps).sample\n",
        "loss = F.mse_loss(noise_pred, noise)"
      ],
      "metadata": {
        "id": "LQDrE-PEE2AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)"
      ],
      "metadata": {
        "id": "FYT6Qc8tE3m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=config.lr_warmup_steps,\n",
        "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
        ")"
      ],
      "metadata": {
        "id": "jMMCYBPTE53n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DDPMPipeline\n",
        "import os\n",
        "import math\n",
        "\n",
        "def make_grid(images, rows, cols):\n",
        "  w, h = images[0].size\n",
        "  grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "  for i, image in enumerate(images):\n",
        "      grid.paste(image, box=(i%cols*w, i//cols*h))\n",
        "  return grid\n",
        "\n",
        "def evaluate(config, epoch, pipeline):\n",
        "    # Sample some images from random noise (this is the backward diffusion process).\n",
        "    # The default pipeline output type is `List[PIL.Image]`\n",
        "    images = pipeline(\n",
        "        batch_size = config.eval_batch_size, \n",
        "        generator=torch.manual_seed(config.seed),\n",
        "    )['sample']\n",
        "\n",
        "    # Make a grid out of the images\n",
        "    image_grid = make_grid(images, rows=4, cols=4)\n",
        "\n",
        "    # Save the images\n",
        "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
      ],
      "metadata": {
        "id": "9wFoVdRTE7or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "from diffusers.hub_utils import init_git_repo, push_to_hub\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
        "    # Initialize accelerator and tensorboard logging\n",
        "    accelerator = Accelerator(\n",
        "        mixed_precision=config.mixed_precision,\n",
        "        gradient_accumulation_steps=config.gradient_accumulation_steps, \n",
        "        log_with=\"tensorboard\",\n",
        "        logging_dir=os.path.join(config.output_dir, \"logs\")\n",
        "    )\n",
        "    if accelerator.is_main_process:\n",
        "        if config.push_to_hub:\n",
        "            repo = init_git_repo(config, at_init=True)\n",
        "        accelerator.init_trackers(\"train_example\")\n",
        "    \n",
        "    # Prepare everything\n",
        "    # There is no specific order to remember, you just need to unpack the \n",
        "    # objects in the same order you gave them to the prepare method.\n",
        "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "    \n",
        "    global_step = 0\n",
        "\n",
        "    # Now you train the model\n",
        "    for epoch in range(config.num_epochs):\n",
        "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
        "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            clean_images = batch['images']\n",
        "            # Sample noise to add to the images\n",
        "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
        "            bs = clean_images.shape[0]\n",
        "\n",
        "            # Sample a random timestep for each image\n",
        "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()\n",
        "\n",
        "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
        "            # (this is the forward diffusion process)\n",
        "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "            \n",
        "            with accelerator.accumulate(model):\n",
        "                # Predict the noise residual\n",
        "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
        "                loss = F.mse_loss(noise_pred, noise)\n",
        "                accelerator.backward(loss)\n",
        "\n",
        "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "            \n",
        "            progress_bar.update(1)\n",
        "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            accelerator.log(logs, step=global_step)\n",
        "            global_step += 1\n",
        "\n",
        "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
        "        if accelerator.is_main_process:\n",
        "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
        "\n",
        "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
        "              evaluate(config, epoch, pipeline)\n",
        "\n",
        "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
        "              if config.push_to_hub:\n",
        "                push_to_hub(config, pipeline, repo, commit_message=f\"Epoch {epoch}\", blocking=True)\n",
        "              else:\n",
        "                pipeline.save_pretrained(config.output_dir)\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "_uQv8ZeZE-JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import notebook_launcher\n",
        "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
        "\n",
        "notebook_launcher(train_loop, args, num_processes=2)"
      ],
      "metadata": {
        "id": "Nhqd_7SvFERl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}